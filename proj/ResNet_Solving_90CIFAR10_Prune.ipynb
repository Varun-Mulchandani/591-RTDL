{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ac6b4b804c314bf694c42651b4b9118c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61338edd01a84277b6224d2600ec0d52",
              "IPY_MODEL_508365707b134f369be723721073524b",
              "IPY_MODEL_a805f15a2ec64779b21be20145988792"
            ],
            "layout": "IPY_MODEL_5ffe6d700a464544b991281ee0c7a325"
          }
        },
        "61338edd01a84277b6224d2600ec0d52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bfe0001b7ef49039a6572315393045c",
            "placeholder": "​",
            "style": "IPY_MODEL_7f410478cf244ddea5f9eacac2a3278a",
            "value": "100%"
          }
        },
        "508365707b134f369be723721073524b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7113c58ea7814e099424d89f4f8911fd",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc83fef426fb4c88925e7e5d7a1ece7c",
            "value": 170498071
          }
        },
        "a805f15a2ec64779b21be20145988792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8944274726d46f4bc1c8ae17954e27e",
            "placeholder": "​",
            "style": "IPY_MODEL_6c5a5769535f4c5691df0e6777bcb041",
            "value": " 170498071/170498071 [00:01&lt;00:00, 95868467.65it/s]"
          }
        },
        "5ffe6d700a464544b991281ee0c7a325": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bfe0001b7ef49039a6572315393045c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f410478cf244ddea5f9eacac2a3278a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7113c58ea7814e099424d89f4f8911fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc83fef426fb4c88925e7e5d7a1ece7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8944274726d46f4bc1c8ae17954e27e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c5a5769535f4c5691df0e6777bcb041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varun-Mulchandani/591-RTDL/blob/main/proj/ResNet_Solving_90CIFAR10_Prune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHhnkev8ZqDh",
        "outputId": "19649cf1-b6af-46d0-a9ea-7b1230488f9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dU7vMF7xcBk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torchvision.transforms import Compose, PILToTensor, ToPILImage"
      ],
      "metadata": {
        "id": "BlxWOkOhpgH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'"
      ],
      "metadata": {
        "id": "JFszFVBYOp0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151,
          "referenced_widgets": [
            "ac6b4b804c314bf694c42651b4b9118c",
            "61338edd01a84277b6224d2600ec0d52",
            "508365707b134f369be723721073524b",
            "a805f15a2ec64779b21be20145988792",
            "5ffe6d700a464544b991281ee0c7a325",
            "1bfe0001b7ef49039a6572315393045c",
            "7f410478cf244ddea5f9eacac2a3278a",
            "7113c58ea7814e099424d89f4f8911fd",
            "fc83fef426fb4c88925e7e5d7a1ece7c",
            "c8944274726d46f4bc1c8ae17954e27e",
            "6c5a5769535f4c5691df0e6777bcb041"
          ]
        },
        "id": "TEdGQJj_xgXg",
        "outputId": "35f83c5d-2d58-4990-b063-350036091c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac6b4b804c314bf694c42651b4b9118c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "torch.manual_seed(1)\n",
        "torch.cuda.manual_seed(1)\n",
        "net = ResNet(BasicBlock, [2, 2, 2, 2]).to(device)"
      ],
      "metadata": {
        "id": "sAIhQKd0xnAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "cPQOLfGexpdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_required = False\n",
        "\n",
        "\n",
        "if training_required:\n",
        "  for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = net(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "              print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "              running_loss = 0.0\n",
        "\n",
        "  print('Finished Training')\n",
        "else:\n",
        "  net = torch.load('/content/drive/MyDrive/res.pt', map_location = torch.device('cpu'))"
      ],
      "metadata": {
        "id": "u2SOcPpMxrSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(net, '/content/drive/MyDrive/res.pt')"
      ],
      "metadata": {
        "id": "KszpfMc8QehN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "images, labels = next(dataiter)\n",
        "images, labels = next(dataiter)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# print images\n",
        "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
      ],
      "metadata": {
        "id": "-lAZy2OuxseH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b43512b-da35-474f-83df-b5cc067640be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GroundTruth:  dog   horse truck ship \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # print(images)\n",
        "# print(images.size())"
      ],
      "metadata": {
        "id": "ubDGqiMFaVz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torchvision.transforms import Compose, PILToTensor, ToPILImage\n",
        "\n",
        "img = images[1]\n",
        "\n",
        "img = img + (0.01**0.5)*torch.randn(3, 32, 32)\n",
        "\n",
        "trans = transforms.ToPILImage()\n",
        "img1 = trans(img)\n",
        "img2 = trans(images[1])\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Variance 0.01\")\n",
        "plt.imshow(img1)\n",
        "plt.subplot(122)\n",
        "plt.title(\"Variance 0\")\n",
        "plt.imshow(img2)"
      ],
      "metadata": {
        "id": "-k0jIqsZnrQg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "f04529ce-7adf-465c-fc43-0dedc9ed8679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7829b433a0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x504 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all_images = []\n",
        "# for i in range(len(images)):\n",
        "#   img = images[i] + (0.001**0.5)*torch.randn(3, 32, 32)\n",
        "#   all_images.append(img)\n",
        "# all_images = torch.stack(all_images)"
      ],
      "metadata": {
        "id": "Nzi77ZoxX2So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_images.size()"
      ],
      "metadata": {
        "id": "im-jeNF9feKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correct = 0\n",
        "# total = 0\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for data in testloader:\n",
        "#         images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "#         outputs = net(images)\n",
        "        \n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "id": "lOGplkh6zJED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.utils.prune as prune "
      ],
      "metadata": {
        "id": "geQRO_np2Jxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First Prune - 90%\n",
        "encoder1_prune3 = type(net)(BasicBlock, [2, 2, 2, 2]).to(device)\n",
        "encoder1_prune3.load_state_dict(net.state_dict())\n",
        "\n",
        "print('Global Sparsity for Encoder: {:.2f}%'.format(\n",
        "    100. * float(\n",
        "        torch.sum(encoder1_prune3.conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer1[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer1[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer1[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer1[0].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[0].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer1[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer1[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer1[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer1[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[0].bn2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[0].shortcut[0].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer2[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer2[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[0].bn2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[0].shortcut[0].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer3[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer3[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[0].bn2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[0].shortcut[0].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer4[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer4[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.linear.weight == 0)\n",
        "    )\n",
        "    / float(encoder1_prune3.conv1.weight.nelement()\n",
        "    + encoder1_prune3.bn1.weight.nelement()\n",
        "    + encoder1_prune3.layer1[0].conv1.weight.nelement()\n",
        "    + encoder1_prune3.layer1[0].bn1.weight.nelement()\n",
        "    + encoder1_prune3.layer1[0].conv2.weight.nelement()\n",
        "    + encoder1_prune3.layer1[0].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer1[0].shortcut.weight.nelement()\n",
        "    + encoder1_prune3.layer1[1].conv1.weight.nelement()\n",
        "    + encoder1_prune3.layer1[1].bn1.weight.nelement()\n",
        "    + encoder1_prune3.layer1[1].conv2.weight.nelement()\n",
        "    + encoder1_prune3.layer1[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer1[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune3.layer2[0].conv1.weight.nelement()\n",
        "    + encoder1_prune3.layer2[0].bn1.weight.nelement()\n",
        "    + encoder1_prune3.layer2[0].conv2.weight.nelement()\n",
        "    + encoder1_prune3.layer2[0].bn2.weight.nelement()\n",
        "    + encoder1_prune3.layer2[0].shortcut[0].weight.nelement()\n",
        "    + encoder1_prune3.layer2[0].shortcut[1].weight.nelement()\n",
        "    + encoder1_prune3.layer2[1].conv1.weight.nelement()\n",
        "    + encoder1_prune3.layer2[1].bn1.weight.nelement()\n",
        "    + encoder1_prune3.layer2[1].conv2.weight.nelement()\n",
        "    + encoder1_prune3.layer2[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer2[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune3.layer3[0].conv1.weight.nelement()\n",
        "    + encoder1_prune3.layer3[0].bn1.weight.nelement()\n",
        "    + encoder1_prune3.layer3[0].conv2.weight.nelement()\n",
        "    + encoder1_prune3.layer3[0].bn2.weight.nelement()\n",
        "    + encoder1_prune3.layer3[0].shortcut[0].weight.nelement()\n",
        "    + encoder1_prune3.layer3[0].shortcut[1].weight.nelement()\n",
        "    + encoder1_prune3.layer3[1].conv1.weight.nelement()\n",
        "    + encoder1_prune3.layer3[1].bn1.weight.nelement()\n",
        "    + encoder1_prune3.layer3[1].conv2.weight.nelement()\n",
        "    + encoder1_prune3.layer3[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer3[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune3.layer4[0].conv1.weight.nelement()\n",
        "    + encoder1_prune3.layer4[0].bn1.weight.nelement()\n",
        "    + encoder1_prune3.layer4[0].conv2.weight.nelement()\n",
        "    + encoder1_prune3.layer4[0].bn2.weight.nelement()\n",
        "    + encoder1_prune3.layer4[0].shortcut[0].weight.nelement()\n",
        "    + encoder1_prune3.layer4[0].shortcut[1].weight.nelement()\n",
        "    + encoder1_prune3.layer4[1].conv1.weight.nelement()\n",
        "    + encoder1_prune3.layer4[1].bn1.weight.nelement()\n",
        "    + encoder1_prune3.layer4[1].conv2.weight.nelement()\n",
        "    + encoder1_prune3.layer4[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer4[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune3.linear.weight.nelement()\n",
        "    )\n",
        ")) \n",
        "\n",
        "parameters_to_prune_encoder = (\n",
        "    (encoder1_prune3.conv1, 'weight'),\n",
        "    (encoder1_prune3.bn1, 'weight'),\n",
        "    (encoder1_prune3.layer1[0].conv1, 'weight'),\n",
        "    (encoder1_prune3.layer1[0].bn1, 'weight'),\n",
        "    (encoder1_prune3.layer1[0].conv2, 'weight'),\n",
        "    (encoder1_prune3.layer1[0].bn2, 'weight'),\n",
        "    (encoder1_prune3.layer1[1].conv1, 'weight'),\n",
        "    (encoder1_prune3.layer1[1].bn1, 'weight'),\n",
        "    (encoder1_prune3.layer1[1].conv2, 'weight'),\n",
        "    (encoder1_prune3.layer1[1].bn2, 'weight'),\n",
        "    (encoder1_prune3.layer2[0].conv1, 'weight'),\n",
        "    (encoder1_prune3.layer2[0].bn1, 'weight'),\n",
        "    (encoder1_prune3.layer2[0].conv2, 'weight'),\n",
        "    (encoder1_prune3.layer2[0].bn2, 'weight'),\n",
        "    (encoder1_prune3.layer2[0].shortcut[0], 'weight'),\n",
        "    (encoder1_prune3.layer2[0].shortcut[1], 'weight'),\n",
        "    (encoder1_prune3.layer2[1].conv1, 'weight'),\n",
        "    (encoder1_prune3.layer2[1].bn1, 'weight'),\n",
        "    (encoder1_prune3.layer2[1].conv2, 'weight'),\n",
        "    (encoder1_prune3.layer2[1].bn2, 'weight'),\n",
        "    (encoder1_prune3.layer3[0].conv1, 'weight'),\n",
        "    (encoder1_prune3.layer3[0].bn1, 'weight'),\n",
        "    (encoder1_prune3.layer3[0].conv2, 'weight'),\n",
        "    (encoder1_prune3.layer3[0].bn2, 'weight'),\n",
        "    (encoder1_prune3.layer3[0].shortcut[0], 'weight'),\n",
        "    (encoder1_prune3.layer3[0].shortcut[1], 'weight'),\n",
        "    (encoder1_prune3.layer3[1].conv1, 'weight'),\n",
        "    (encoder1_prune3.layer3[1].bn1, 'weight'),\n",
        "    (encoder1_prune3.layer3[1].conv2, 'weight'),\n",
        "    (encoder1_prune3.layer3[1].bn2, 'weight'),\n",
        "    (encoder1_prune3.layer4[0].conv1, 'weight'),\n",
        "    (encoder1_prune3.layer4[0].bn1, 'weight'),\n",
        "    (encoder1_prune3.layer4[0].conv2, 'weight'),\n",
        "    (encoder1_prune3.layer4[0].bn2, 'weight'),\n",
        "    (encoder1_prune3.layer4[0].shortcut[0], 'weight'),\n",
        "    (encoder1_prune3.layer4[0].shortcut[1], 'weight'),\n",
        "    (encoder1_prune3.layer4[1].conv1, 'weight'),\n",
        "    (encoder1_prune3.layer4[1].bn1, 'weight'),\n",
        "    (encoder1_prune3.layer4[1].conv2, 'weight'),\n",
        "    (encoder1_prune3.layer4[1].bn2, 'weight'),\n",
        "    (encoder1_prune3.linear, 'weight'),\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune_encoder,\n",
        "    pruning_method = prune.L1Unstructured,\n",
        "    amount = 0.9,\n",
        ")\n",
        "\n",
        "print('Global Sparsity for Encoder: {:.2f}%'.format(\n",
        "    100. * float(\n",
        "        torch.sum(encoder1_prune3.conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer1[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer1[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer1[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer1[0].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[0].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer1[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer1[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer1[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer1[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[0].bn2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[0].shortcut[0].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer2[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer2[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer2[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[0].bn2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[0].shortcut[0].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer3[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer3[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer3[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[0].bn2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[0].shortcut[0].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune3.layer4[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer4[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer4[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune3.linear.weight == 0)\n",
        "    )\n",
        "    / float(encoder1_prune3.conv1.weight.nelement()\n",
        "    + encoder1_prune3.bn1.weight.nelement()\n",
        "    + encoder1_prune3.layer1[0].conv1.weight.nelement()\n",
        "    + encoder1_prune3.layer1[0].bn1.weight.nelement()\n",
        "    + encoder1_prune3.layer1[0].conv2.weight.nelement()\n",
        "    + encoder1_prune3.layer1[0].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer1[0].shortcut.weight.nelement()\n",
        "    + encoder1_prune3.layer1[1].conv1.weight.nelement()\n",
        "    + encoder1_prune3.layer1[1].bn1.weight.nelement()\n",
        "    + encoder1_prune3.layer1[1].conv2.weight.nelement()\n",
        "    + encoder1_prune3.layer1[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer1[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune3.layer2[0].conv1.weight.nelement()\n",
        "    + encoder1_prune3.layer2[0].bn1.weight.nelement()\n",
        "    + encoder1_prune3.layer2[0].conv2.weight.nelement()\n",
        "    + encoder1_prune3.layer2[0].bn2.weight.nelement()\n",
        "    + encoder1_prune3.layer2[0].shortcut[0].weight.nelement()\n",
        "    + encoder1_prune3.layer2[0].shortcut[1].weight.nelement()\n",
        "    + encoder1_prune3.layer2[1].conv1.weight.nelement()\n",
        "    + encoder1_prune3.layer2[1].bn1.weight.nelement()\n",
        "    + encoder1_prune3.layer2[1].conv2.weight.nelement()\n",
        "    + encoder1_prune3.layer2[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer2[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune3.layer3[0].conv1.weight.nelement()\n",
        "    + encoder1_prune3.layer3[0].bn1.weight.nelement()\n",
        "    + encoder1_prune3.layer3[0].conv2.weight.nelement()\n",
        "    + encoder1_prune3.layer3[0].bn2.weight.nelement()\n",
        "    + encoder1_prune3.layer3[0].shortcut[0].weight.nelement()\n",
        "    + encoder1_prune3.layer3[0].shortcut[1].weight.nelement()\n",
        "    + encoder1_prune3.layer3[1].conv1.weight.nelement()\n",
        "    + encoder1_prune3.layer3[1].bn1.weight.nelement()\n",
        "    + encoder1_prune3.layer3[1].conv2.weight.nelement()\n",
        "    + encoder1_prune3.layer3[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer3[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune3.layer4[0].conv1.weight.nelement()\n",
        "    + encoder1_prune3.layer4[0].bn1.weight.nelement()\n",
        "    + encoder1_prune3.layer4[0].conv2.weight.nelement()\n",
        "    + encoder1_prune3.layer4[0].bn2.weight.nelement()\n",
        "    + encoder1_prune3.layer4[0].shortcut[0].weight.nelement()\n",
        "    + encoder1_prune3.layer4[0].shortcut[1].weight.nelement()\n",
        "    + encoder1_prune3.layer4[1].conv1.weight.nelement()\n",
        "    + encoder1_prune3.layer4[1].bn1.weight.nelement()\n",
        "    + encoder1_prune3.layer4[1].conv2.weight.nelement()\n",
        "    + encoder1_prune3.layer4[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer4[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune3.linear.weight.nelement()\n",
        "    )\n",
        ")) \n",
        "\n",
        "\n",
        "encoder1_prune4 = type(net)(BasicBlock, [2, 2, 2, 2]).to(device)\n",
        "encoder1_prune4.load_state_dict(net.state_dict())\n",
        "\n",
        "print('Global Sparsity for Encoder: {:.2f}%'.format(\n",
        "    100. * float(\n",
        "        torch.sum(encoder1_prune3.conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer1[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer1[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer1[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer1[0].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[0].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer1[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer1[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer1[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer1[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[0].bn2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[0].shortcut[0].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer2[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer2[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[0].bn2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[0].shortcut[0].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer3[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer3[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[0].bn2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[0].shortcut[0].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer4[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer4[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.linear.weight == 0)\n",
        "    )\n",
        "    / float(encoder1_prune4.conv1.weight.nelement()\n",
        "    + encoder1_prune4.bn1.weight.nelement()\n",
        "    + encoder1_prune4.layer1[0].conv1.weight.nelement()\n",
        "    + encoder1_prune4.layer1[0].bn1.weight.nelement()\n",
        "    + encoder1_prune4.layer1[0].conv2.weight.nelement()\n",
        "    + encoder1_prune4.layer1[0].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer1[0].shortcut.weight.nelement()\n",
        "    + encoder1_prune4.layer1[1].conv1.weight.nelement()\n",
        "    + encoder1_prune4.layer1[1].bn1.weight.nelement()\n",
        "    + encoder1_prune4.layer1[1].conv2.weight.nelement()\n",
        "    + encoder1_prune4.layer1[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer1[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune4.layer2[0].conv1.weight.nelement()\n",
        "    + encoder1_prune4.layer2[0].bn1.weight.nelement()\n",
        "    + encoder1_prune4.layer2[0].conv2.weight.nelement()\n",
        "    + encoder1_prune4.layer2[0].bn2.weight.nelement()\n",
        "    + encoder1_prune4.layer2[0].shortcut[0].weight.nelement()\n",
        "    + encoder1_prune4.layer2[0].shortcut[1].weight.nelement()\n",
        "    + encoder1_prune4.layer2[1].conv1.weight.nelement()\n",
        "    + encoder1_prune4.layer2[1].bn1.weight.nelement()\n",
        "    + encoder1_prune4.layer2[1].conv2.weight.nelement()\n",
        "    + encoder1_prune4.layer2[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer2[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune4.layer3[0].conv1.weight.nelement()\n",
        "    + encoder1_prune4.layer3[0].bn1.weight.nelement()\n",
        "    + encoder1_prune4.layer3[0].conv2.weight.nelement()\n",
        "    + encoder1_prune4.layer3[0].bn2.weight.nelement()\n",
        "    + encoder1_prune4.layer3[0].shortcut[0].weight.nelement()\n",
        "    + encoder1_prune4.layer3[0].shortcut[1].weight.nelement()\n",
        "    + encoder1_prune4.layer3[1].conv1.weight.nelement()\n",
        "    + encoder1_prune4.layer3[1].bn1.weight.nelement()\n",
        "    + encoder1_prune4.layer3[1].conv2.weight.nelement()\n",
        "    + encoder1_prune4.layer3[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer3[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune4.layer4[0].conv1.weight.nelement()\n",
        "    + encoder1_prune4.layer4[0].bn1.weight.nelement()\n",
        "    + encoder1_prune4.layer4[0].conv2.weight.nelement()\n",
        "    + encoder1_prune4.layer4[0].bn2.weight.nelement()\n",
        "    + encoder1_prune4.layer4[0].shortcut[0].weight.nelement()\n",
        "    + encoder1_prune4.layer4[0].shortcut[1].weight.nelement()\n",
        "    + encoder1_prune4.layer4[1].conv1.weight.nelement()\n",
        "    + encoder1_prune4.layer4[1].bn1.weight.nelement()\n",
        "    + encoder1_prune4.layer4[1].conv2.weight.nelement()\n",
        "    + encoder1_prune4.layer4[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer4[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune4.linear.weight.nelement()\n",
        "    )\n",
        ")) \n",
        "\n",
        "parameters_to_prune_encoder = (\n",
        "    (encoder1_prune4.conv1, 'weight'),\n",
        "    (encoder1_prune4.bn1, 'weight'),\n",
        "    (encoder1_prune4.layer1[0].conv1, 'weight'),\n",
        "    (encoder1_prune4.layer1[0].bn1, 'weight'),\n",
        "    (encoder1_prune4.layer1[0].conv2, 'weight'),\n",
        "    (encoder1_prune4.layer1[0].bn2, 'weight'),\n",
        "    (encoder1_prune4.layer1[1].conv1, 'weight'),\n",
        "    (encoder1_prune4.layer1[1].bn1, 'weight'),\n",
        "    (encoder1_prune4.layer1[1].conv2, 'weight'),\n",
        "    (encoder1_prune4.layer1[1].bn2, 'weight'),\n",
        "    (encoder1_prune4.layer2[0].conv1, 'weight'),\n",
        "    (encoder1_prune4.layer2[0].bn1, 'weight'),\n",
        "    (encoder1_prune4.layer2[0].conv2, 'weight'),\n",
        "    (encoder1_prune4.layer2[0].bn2, 'weight'),\n",
        "    (encoder1_prune4.layer2[0].shortcut[0], 'weight'),\n",
        "    (encoder1_prune4.layer2[0].shortcut[1], 'weight'),\n",
        "    (encoder1_prune4.layer2[1].conv1, 'weight'),\n",
        "    (encoder1_prune4.layer2[1].bn1, 'weight'),\n",
        "    (encoder1_prune4.layer2[1].conv2, 'weight'),\n",
        "    (encoder1_prune4.layer2[1].bn2, 'weight'),\n",
        "    (encoder1_prune4.layer3[0].conv1, 'weight'),\n",
        "    (encoder1_prune4.layer3[0].bn1, 'weight'),\n",
        "    (encoder1_prune4.layer3[0].conv2, 'weight'),\n",
        "    (encoder1_prune4.layer3[0].bn2, 'weight'),\n",
        "    (encoder1_prune4.layer3[0].shortcut[0], 'weight'),\n",
        "    (encoder1_prune4.layer3[0].shortcut[1], 'weight'),\n",
        "    (encoder1_prune4.layer3[1].conv1, 'weight'),\n",
        "    (encoder1_prune4.layer3[1].bn1, 'weight'),\n",
        "    (encoder1_prune4.layer3[1].conv2, 'weight'),\n",
        "    (encoder1_prune4.layer3[1].bn2, 'weight'),\n",
        "    (encoder1_prune4.layer4[0].conv1, 'weight'),\n",
        "    (encoder1_prune4.layer4[0].bn1, 'weight'),\n",
        "    (encoder1_prune4.layer4[0].conv2, 'weight'),\n",
        "    (encoder1_prune4.layer4[0].bn2, 'weight'),\n",
        "    (encoder1_prune4.layer4[0].shortcut[0], 'weight'),\n",
        "    (encoder1_prune4.layer4[0].shortcut[1], 'weight'),\n",
        "    (encoder1_prune4.layer4[1].conv1, 'weight'),\n",
        "    (encoder1_prune4.layer4[1].bn1, 'weight'),\n",
        "    (encoder1_prune4.layer4[1].conv2, 'weight'),\n",
        "    (encoder1_prune4.layer4[1].bn2, 'weight'),\n",
        "    (encoder1_prune4.linear, 'weight'),\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune_encoder,\n",
        "    pruning_method = prune.L1Unstructured,\n",
        "    amount = 0.9,\n",
        ")\n",
        "\n",
        "print('Global Sparsity for Encoder: {:.2f}%'.format(\n",
        "    100. * float(\n",
        "        torch.sum(encoder1_prune4.conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer1[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer1[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer1[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer1[0].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[0].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer1[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer1[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer1[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer1[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[0].bn2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[0].shortcut[0].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer2[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer2[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer2[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[0].bn2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[0].shortcut[0].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer3[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer3[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer3[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[0].bn2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[0].shortcut[0].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune4.layer4[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer4[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer4[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune4.linear.weight == 0)\n",
        "    )\n",
        "    / float(encoder1_prune4.conv1.weight.nelement()\n",
        "    + encoder1_prune4.bn1.weight.nelement()\n",
        "    + encoder1_prune4.layer1[0].conv1.weight.nelement()\n",
        "    + encoder1_prune4.layer1[0].bn1.weight.nelement()\n",
        "    + encoder1_prune4.layer1[0].conv2.weight.nelement()\n",
        "    + encoder1_prune4.layer1[0].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer1[0].shortcut.weight.nelement()\n",
        "    + encoder1_prune4.layer1[1].conv1.weight.nelement()\n",
        "    + encoder1_prune4.layer1[1].bn1.weight.nelement()\n",
        "    + encoder1_prune4.layer1[1].conv2.weight.nelement()\n",
        "    + encoder1_prune4.layer1[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer1[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune4.layer2[0].conv1.weight.nelement()\n",
        "    + encoder1_prune4.layer2[0].bn1.weight.nelement()\n",
        "    + encoder1_prune4.layer2[0].conv2.weight.nelement()\n",
        "    + encoder1_prune4.layer2[0].bn2.weight.nelement()\n",
        "    + encoder1_prune4.layer2[0].shortcut[0].weight.nelement()\n",
        "    + encoder1_prune4.layer2[0].shortcut[1].weight.nelement()\n",
        "    + encoder1_prune4.layer2[1].conv1.weight.nelement()\n",
        "    + encoder1_prune4.layer2[1].bn1.weight.nelement()\n",
        "    + encoder1_prune4.layer2[1].conv2.weight.nelement()\n",
        "    + encoder1_prune4.layer2[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer2[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune4.layer3[0].conv1.weight.nelement()\n",
        "    + encoder1_prune4.layer3[0].bn1.weight.nelement()\n",
        "    + encoder1_prune4.layer3[0].conv2.weight.nelement()\n",
        "    + encoder1_prune4.layer3[0].bn2.weight.nelement()\n",
        "    + encoder1_prune4.layer3[0].shortcut[0].weight.nelement()\n",
        "    + encoder1_prune4.layer3[0].shortcut[1].weight.nelement()\n",
        "    + encoder1_prune4.layer3[1].conv1.weight.nelement()\n",
        "    + encoder1_prune4.layer3[1].bn1.weight.nelement()\n",
        "    + encoder1_prune4.layer3[1].conv2.weight.nelement()\n",
        "    + encoder1_prune4.layer3[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer3[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune4.layer4[0].conv1.weight.nelement()\n",
        "    + encoder1_prune4.layer4[0].bn1.weight.nelement()\n",
        "    + encoder1_prune4.layer4[0].conv2.weight.nelement()\n",
        "    + encoder1_prune4.layer4[0].bn2.weight.nelement()\n",
        "    + encoder1_prune4.layer4[0].shortcut[0].weight.nelement()\n",
        "    + encoder1_prune4.layer4[0].shortcut[1].weight.nelement()\n",
        "    + encoder1_prune4.layer4[1].conv1.weight.nelement()\n",
        "    + encoder1_prune4.layer4[1].bn1.weight.nelement()\n",
        "    + encoder1_prune4.layer4[1].conv2.weight.nelement()\n",
        "    + encoder1_prune4.layer4[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer4[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune4.linear.weight.nelement()\n",
        "    )\n",
        ")) \n",
        "\n",
        "encoder1_prune5 = type(net)(BasicBlock, [2, 2, 2, 2]).to(device)\n",
        "encoder1_prune5.load_state_dict(net.state_dict())\n",
        "\n",
        "print('Global Sparsity for Encoder: {:.2f}%'.format(\n",
        "    100. * float(\n",
        "        torch.sum(encoder1_prune5.conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer1[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer1[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer1[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer1[0].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[0].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer1[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer1[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer1[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer1[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[0].bn2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[0].shortcut[0].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer2[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer2[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[0].bn2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[0].shortcut[0].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer3[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer3[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[0].bn2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[0].shortcut[0].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer4[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer4[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.linear.weight == 0)\n",
        "    )\n",
        "    / float(encoder1_prune5.conv1.weight.nelement()\n",
        "    + encoder1_prune5.bn1.weight.nelement()\n",
        "    + encoder1_prune5.layer1[0].conv1.weight.nelement()\n",
        "    + encoder1_prune5.layer1[0].bn1.weight.nelement()\n",
        "    + encoder1_prune5.layer1[0].conv2.weight.nelement()\n",
        "    + encoder1_prune5.layer1[0].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer1[0].shortcut.weight.nelement()\n",
        "    + encoder1_prune5.layer1[1].conv1.weight.nelement()\n",
        "    + encoder1_prune5.layer1[1].bn1.weight.nelement()\n",
        "    + encoder1_prune5.layer1[1].conv2.weight.nelement()\n",
        "    + encoder1_prune5.layer1[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer1[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune5.layer2[0].conv1.weight.nelement()\n",
        "    + encoder1_prune5.layer2[0].bn1.weight.nelement()\n",
        "    + encoder1_prune5.layer2[0].conv2.weight.nelement()\n",
        "    + encoder1_prune5.layer2[0].bn2.weight.nelement()\n",
        "    + encoder1_prune5.layer2[0].shortcut[0].weight.nelement()\n",
        "    + encoder1_prune5.layer2[0].shortcut[1].weight.nelement()\n",
        "    + encoder1_prune5.layer2[1].conv1.weight.nelement()\n",
        "    + encoder1_prune5.layer2[1].bn1.weight.nelement()\n",
        "    + encoder1_prune5.layer2[1].conv2.weight.nelement()\n",
        "    + encoder1_prune5.layer2[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer2[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune5.layer3[0].conv1.weight.nelement()\n",
        "    + encoder1_prune5.layer3[0].bn1.weight.nelement()\n",
        "    + encoder1_prune5.layer3[0].conv2.weight.nelement()\n",
        "    + encoder1_prune5.layer3[0].bn2.weight.nelement()\n",
        "    + encoder1_prune5.layer3[0].shortcut[0].weight.nelement()\n",
        "    + encoder1_prune5.layer3[0].shortcut[1].weight.nelement()\n",
        "    + encoder1_prune5.layer3[1].conv1.weight.nelement()\n",
        "    + encoder1_prune5.layer3[1].bn1.weight.nelement()\n",
        "    + encoder1_prune5.layer3[1].conv2.weight.nelement()\n",
        "    + encoder1_prune5.layer3[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer3[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune5.layer4[0].conv1.weight.nelement()\n",
        "    + encoder1_prune5.layer4[0].bn1.weight.nelement()\n",
        "    + encoder1_prune5.layer4[0].conv2.weight.nelement()\n",
        "    + encoder1_prune5.layer4[0].bn2.weight.nelement()\n",
        "    + encoder1_prune5.layer4[0].shortcut[0].weight.nelement()\n",
        "    + encoder1_prune5.layer4[0].shortcut[1].weight.nelement()\n",
        "    + encoder1_prune5.layer4[1].conv1.weight.nelement()\n",
        "    + encoder1_prune5.layer4[1].bn1.weight.nelement()\n",
        "    + encoder1_prune5.layer4[1].conv2.weight.nelement()\n",
        "    + encoder1_prune5.layer4[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer4[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune5.linear.weight.nelement()\n",
        "    )\n",
        ")) \n",
        "\n",
        "parameters_to_prune_encoder = (\n",
        "    (encoder1_prune5.conv1, 'weight'),\n",
        "    (encoder1_prune5.bn1, 'weight'),\n",
        "    (encoder1_prune5.layer1[0].conv1, 'weight'),\n",
        "    (encoder1_prune5.layer1[0].bn1, 'weight'),\n",
        "    (encoder1_prune5.layer1[0].conv2, 'weight'),\n",
        "    (encoder1_prune5.layer1[0].bn2, 'weight'),\n",
        "    (encoder1_prune5.layer1[1].conv1, 'weight'),\n",
        "    (encoder1_prune5.layer1[1].bn1, 'weight'),\n",
        "    (encoder1_prune5.layer1[1].conv2, 'weight'),\n",
        "    (encoder1_prune5.layer1[1].bn2, 'weight'),\n",
        "    (encoder1_prune5.layer2[0].conv1, 'weight'),\n",
        "    (encoder1_prune5.layer2[0].bn1, 'weight'),\n",
        "    (encoder1_prune5.layer2[0].conv2, 'weight'),\n",
        "    (encoder1_prune5.layer2[0].bn2, 'weight'),\n",
        "    (encoder1_prune5.layer2[0].shortcut[0], 'weight'),\n",
        "    (encoder1_prune5.layer2[0].shortcut[1], 'weight'),\n",
        "    (encoder1_prune5.layer2[1].conv1, 'weight'),\n",
        "    (encoder1_prune5.layer2[1].bn1, 'weight'),\n",
        "    (encoder1_prune5.layer2[1].conv2, 'weight'),\n",
        "    (encoder1_prune5.layer2[1].bn2, 'weight'),\n",
        "    (encoder1_prune5.layer3[0].conv1, 'weight'),\n",
        "    (encoder1_prune5.layer3[0].bn1, 'weight'),\n",
        "    (encoder1_prune5.layer3[0].conv2, 'weight'),\n",
        "    (encoder1_prune5.layer3[0].bn2, 'weight'),\n",
        "    (encoder1_prune5.layer3[0].shortcut[0], 'weight'),\n",
        "    (encoder1_prune5.layer3[0].shortcut[1], 'weight'),\n",
        "    (encoder1_prune5.layer3[1].conv1, 'weight'),\n",
        "    (encoder1_prune5.layer3[1].bn1, 'weight'),\n",
        "    (encoder1_prune5.layer3[1].conv2, 'weight'),\n",
        "    (encoder1_prune5.layer3[1].bn2, 'weight'),\n",
        "    (encoder1_prune5.layer4[0].conv1, 'weight'),\n",
        "    (encoder1_prune5.layer4[0].bn1, 'weight'),\n",
        "    (encoder1_prune5.layer4[0].conv2, 'weight'),\n",
        "    (encoder1_prune5.layer4[0].bn2, 'weight'),\n",
        "    (encoder1_prune5.layer4[0].shortcut[0], 'weight'),\n",
        "    (encoder1_prune5.layer4[0].shortcut[1], 'weight'),\n",
        "    (encoder1_prune5.layer4[1].conv1, 'weight'),\n",
        "    (encoder1_prune5.layer4[1].bn1, 'weight'),\n",
        "    (encoder1_prune5.layer4[1].conv2, 'weight'),\n",
        "    (encoder1_prune5.layer4[1].bn2, 'weight'),\n",
        "    (encoder1_prune5.linear, 'weight'),\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune_encoder,\n",
        "    pruning_method = prune.L1Unstructured,\n",
        "    amount = 0.9,\n",
        ")\n",
        "\n",
        "print('Global Sparsity for Encoder: {:.2f}%'.format(\n",
        "    100. * float(\n",
        "        torch.sum(encoder1_prune5.conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer1[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer1[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer1[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer1[0].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[0].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer1[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer1[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer1[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer1[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer1[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[0].bn2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[0].shortcut[0].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer2[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer2[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer2[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[0].bn2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[0].shortcut[0].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer3[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer3[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer3[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[0].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[0].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[0].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[0].bn2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[0].shortcut[0].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[0].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[1].conv1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[1].bn1.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[1].conv2.weight == 0)\n",
        "        + torch.sum(encoder1_prune5.layer4[1].bn2.weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer4[1].shortcut[0].weight == 0)\n",
        "        # + torch.sum(encoder1_prune3.layer4[1].shortcut[1].weight == 0)\n",
        "        + torch.sum(encoder1_prune5.linear.weight == 0)\n",
        "    )\n",
        "    / float(encoder1_prune5.conv1.weight.nelement()\n",
        "    + encoder1_prune5.bn1.weight.nelement()\n",
        "    + encoder1_prune5.layer1[0].conv1.weight.nelement()\n",
        "    + encoder1_prune5.layer1[0].bn1.weight.nelement()\n",
        "    + encoder1_prune5.layer1[0].conv2.weight.nelement()\n",
        "    + encoder1_prune5.layer1[0].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer1[0].shortcut.weight.nelement()\n",
        "    + encoder1_prune5.layer1[1].conv1.weight.nelement()\n",
        "    + encoder1_prune5.layer1[1].bn1.weight.nelement()\n",
        "    + encoder1_prune5.layer1[1].conv2.weight.nelement()\n",
        "    + encoder1_prune5.layer1[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer1[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune5.layer2[0].conv1.weight.nelement()\n",
        "    + encoder1_prune5.layer2[0].bn1.weight.nelement()\n",
        "    + encoder1_prune5.layer2[0].conv2.weight.nelement()\n",
        "    + encoder1_prune5.layer2[0].bn2.weight.nelement()\n",
        "    + encoder1_prune5.layer2[0].shortcut[0].weight.nelement()\n",
        "    + encoder1_prune5.layer2[0].shortcut[1].weight.nelement()\n",
        "    + encoder1_prune5.layer2[1].conv1.weight.nelement()\n",
        "    + encoder1_prune5.layer2[1].bn1.weight.nelement()\n",
        "    + encoder1_prune5.layer2[1].conv2.weight.nelement()\n",
        "    + encoder1_prune5.layer2[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer2[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune5.layer3[0].conv1.weight.nelement()\n",
        "    + encoder1_prune5.layer3[0].bn1.weight.nelement()\n",
        "    + encoder1_prune5.layer3[0].conv2.weight.nelement()\n",
        "    + encoder1_prune5.layer3[0].bn2.weight.nelement()\n",
        "    + encoder1_prune5.layer3[0].shortcut[0].weight.nelement()\n",
        "    + encoder1_prune5.layer3[0].shortcut[1].weight.nelement()\n",
        "    + encoder1_prune5.layer3[1].conv1.weight.nelement()\n",
        "    + encoder1_prune5.layer3[1].bn1.weight.nelement()\n",
        "    + encoder1_prune5.layer3[1].conv2.weight.nelement()\n",
        "    + encoder1_prune5.layer3[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer3[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune5.layer4[0].conv1.weight.nelement()\n",
        "    + encoder1_prune5.layer4[0].bn1.weight.nelement()\n",
        "    + encoder1_prune5.layer4[0].conv2.weight.nelement()\n",
        "    + encoder1_prune5.layer4[0].bn2.weight.nelement()\n",
        "    + encoder1_prune5.layer4[0].shortcut[0].weight.nelement()\n",
        "    + encoder1_prune5.layer4[0].shortcut[1].weight.nelement()\n",
        "    + encoder1_prune5.layer4[1].conv1.weight.nelement()\n",
        "    + encoder1_prune5.layer4[1].bn1.weight.nelement()\n",
        "    + encoder1_prune5.layer4[1].conv2.weight.nelement()\n",
        "    + encoder1_prune5.layer4[1].bn2.weight.nelement()\n",
        "    # + encoder1_prune3.layer4[1].shortcut.weight.nelement()\n",
        "    + encoder1_prune5.linear.weight.nelement()\n",
        "    )\n",
        ")) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3NAX3Rq1C9B",
        "outputId": "65e56d15-1e60-4e0c-f6bd-28f6f651eaf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global Sparsity for Encoder: 0.00%\n",
            "Global Sparsity for Encoder: 90.00%\n",
            "Global Sparsity for Encoder: 0.00%\n",
            "Global Sparsity for Encoder: 90.00%\n",
            "Global Sparsity for Encoder: 0.00%\n",
            "Global Sparsity for Encoder: 90.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_required = False\n",
        "\n",
        "if training_required:\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(encoder1_prune3.parameters(), lr=0.001, momentum=0.9)\n",
        "  # other_criterion = nn.CrossEntropyLoss()\n",
        "  for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = encoder1_prune3(inputs)\n",
        "          net_outputs = net(inputs)\n",
        "          # print(outputs)\n",
        "          # print(net_outputs)\n",
        "          # test_loss = other_criterion(outputs, net_outputs)\n",
        "          # print(test_loss)\n",
        "          # print(1/0)\n",
        "          loss = criterion(outputs, labels)\n",
        "          # print(other_loss)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "              print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "              running_loss = 0.0\n",
        "\n",
        "  print('Finished Training')\n",
        "  torch.save(encoder1_prune3, '/content/drive/MyDrive/prune1.pt')\n",
        "else:\n",
        "  encoder1_prune3 = torch.load('/content/drive/MyDrive/prune1.pt', map_location = torch.device('cpu'))"
      ],
      "metadata": {
        "id": "9R9gpAxb7Tld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(encoder1_prune3, '/content/drive/MyDrive/prune1.pt')"
      ],
      "metadata": {
        "id": "e-yx3twwaALj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_required = False\n",
        "\n",
        "if training_required:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(encoder1_prune4.parameters(), lr=0.001, momentum=0.9)\n",
        "  other_criterion = nn.KLDivLoss(reduction = 'mean')\n",
        "  # other_criterion = nn.CrossEntropyLoss()\n",
        "  for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = encoder1_prune4(inputs)\n",
        "          net_outputs = net(inputs)\n",
        "          # print(outputs)\n",
        "          # print(net_outputs)\n",
        "          # test_loss = other_criterion(outputs, net_outputs)\n",
        "          # print(test_loss)\n",
        "          # print(1/0)\n",
        "          loss = criterion(outputs, labels)\n",
        "          other_loss = other_criterion(F.log_softmax(outputs), F.softmax(net_outputs))\n",
        "          # print(other_loss)\n",
        "          loss += other_loss\n",
        "          loss.backward()\n",
        "\n",
        "          torch.nn.utils.clip_grad_norm_(encoder1_prune4.parameters(), max_norm = 2, norm_type = 2)\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "              print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "              running_loss = 0.0\n",
        "\n",
        "  print('Finished Training')\n",
        "  torch.save(encoder1_prune4, '/content/drive/MyDrive/prune2.pt')\n",
        "else:\n",
        "  encoder1_prune4 = torch.load('/content/drive/MyDrive/prune2.pt', map_location = torch.device('cpu'))"
      ],
      "metadata": {
        "id": "ghM75u9_KAaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(encoder1_prune4, '/content/drive/MyDrive/prune2.pt')"
      ],
      "metadata": {
        "id": "d4j4sRSPP-Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_required = False\n",
        "\n",
        "if training_required:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(encoder1_prune5.parameters(), lr=0.0001, momentum=0.9)\n",
        "  other_criterion = nn.KLDivLoss(reduction = 'mean')\n",
        "  # other_criterion = nn.CrossEntropyLoss()\n",
        "  for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          to_add_noise1 = torch.randn(encoder1_prune5.layer1[0].conv1.weight.size())*0.2\n",
        "          to_add_noise2 = torch.randn(encoder1_prune5.layer1[1].conv2.weight.size())*0.2\n",
        "          to_add_noise3 = torch.randn(encoder1_prune5.layer2[0].conv1.weight.size())*0.2\n",
        "          to_add_noise4 = torch.randn(encoder1_prune5.layer2[1].conv2.weight.size())*0.2\n",
        "          to_add_noise5 = torch.randn(encoder1_prune5.layer3[0].conv1.weight.size())*0.2\n",
        "          to_add_noise6 = torch.randn(encoder1_prune5.layer3[1].conv2.weight.size())*0.2\n",
        "          to_add_noise7 = torch.randn(encoder1_prune5.layer4[0].conv1.weight.size())*0.2\n",
        "          to_add_noise8 = torch.randn(encoder1_prune5.layer4[1].conv2.weight.size())*0.2\n",
        "\n",
        "          with torch.no_grad():\n",
        "              encoder1_prune5.layer1[0].conv1.weight.add_(to_add_noise1.to(device))\n",
        "              encoder1_prune5.layer1[1].conv2.weight.add_(to_add_noise2.to(device))\n",
        "              encoder1_prune5.layer2[0].conv1.weight.add_(to_add_noise3.to(device))\n",
        "              encoder1_prune5.layer2[1].conv2.weight.add_(to_add_noise4.to(device))\n",
        "              encoder1_prune5.layer3[0].conv1.weight.add_(to_add_noise5.to(device))\n",
        "              encoder1_prune5.layer3[1].conv2.weight.add_(to_add_noise6.to(device))\n",
        "              encoder1_prune5.layer4[0].conv1.weight.add_(to_add_noise7.to(device))\n",
        "              encoder1_prune5.layer4[1].conv2.weight.add_(to_add_noise8.to(device))\n",
        "\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = encoder1_prune4(inputs)\n",
        "          net_outputs = net(inputs)\n",
        "          # print(outputs)\n",
        "          # print(net_outputs)\n",
        "          # test_loss = other_criterion(outputs, net_outputs)\n",
        "          # print(test_loss)\n",
        "          # print(1/0)\n",
        "          loss = criterion(outputs, labels)\n",
        "          other_loss = other_criterion(F.log_softmax(outputs), F.softmax(net_outputs))\n",
        "          # print(other_loss)\n",
        "          loss += other_loss\n",
        "          loss.backward()\n",
        "\n",
        "          torch.nn.utils.clip_grad_norm_(encoder1_prune4.parameters(), max_norm = 2, norm_type = 2)\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "              print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "              running_loss = 0.0\n",
        "\n",
        "  print('Finished Training')\n",
        "  torch.save(encoder1_prune5, '/content/drive/MyDrive/prune3.pt')\n",
        "else:\n",
        "  encoder1_prune5 = torch.load('/content/drive/MyDrive/prune3.pt', map_location = torch.device('cpu'))"
      ],
      "metadata": {
        "id": "CNc8MwSn5p4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.eval()\n",
        "encoder1_prune3.eval()\n",
        "encoder1_prune4.eval()\n",
        "encoder1_prune5.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xlYDSIhZEG8",
        "outputId": "90a3f643-714f-4185-ba34-fc74c96e78ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "print('TEST ON NO NOISE')\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        outputs = net(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        outputs = encoder1_prune3(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        outputs = encoder1_prune4(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        outputs = encoder1_prune5(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "print('TEST ON NOISY DATA WITH STD OF 0.05')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.05**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = net(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.05**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune3(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.05**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune4(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.05**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune5(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "print('TEST ON NOISY DATA WITH STD OF 0.075')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.075**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = net(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.075**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune3(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.075**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune4(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.075**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune5(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "print('TEST ON NOISY DATA WITH STD OF 0.1')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.1**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = net(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.1**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune3(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.1**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune4(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.1**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune5(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "print('TEST ON NOISY DATA WITH STD OF 0.125')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.125**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = net(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.125**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune3(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.125**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune4(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.125**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune5(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "print('TEST ON NOISY DATA WITH STD OF 0.15')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.15**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = net(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.15**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune3(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.15**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune4(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.15**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune5(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "\n",
        "print('TEST ON NOISY DATA WITH STD OF 0.2')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.2**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = net(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.2**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune3(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.2**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune4(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.2**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune5(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "\n",
        "print('TEST ON NOISY DATA WITH STD OF 0.3')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.3**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = net(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.3**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune3(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.3**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune4(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.3**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune5(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "\n",
        "print('TEST ON NOISY DATA WITH STD OF 0.4')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.4**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = net(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.4**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune3(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.4**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune4(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.4**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune5(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "\n",
        "print('TEST ON NOISY DATA WITH STD OF 0.5')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.5**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = net(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.5**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune3(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.5**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune4(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.5**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune5(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "\n",
        "print('TEST ON NOISY DATA WITH STD OF 0.6')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.6**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = net(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.6**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune3(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.6**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune4(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.6**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune5(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "print('TEST ON NOISY DATA WITH STD OF 0.7')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.7**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = net(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.7**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune3(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.7**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune4(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          torch.cuda.manual_seed(1)\n",
        "          img = images[j] + (0.7**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images)\n",
        "\n",
        "        outputs = encoder1_prune5(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q8QN1b38vjU",
        "outputId": "8c5b60ea-fa75-4b0a-b372-b1a5b38b60df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST ON NO NOISE\n",
            "Accuracy of the network on the 10000 test images: 83 %\n",
            "Accuracy of the network on the 10000 test images: 84 %\n",
            "Accuracy of the network on the 10000 test images: 85 %\n",
            "Accuracy of the network on the 10000 test images: 21 %\n",
            "TEST ON NOISY DATA WITH STD OF 0.05\n",
            "Accuracy of the network on the 10000 test images: 43 %\n",
            "Accuracy of the network on the 10000 test images: 33 %\n",
            "Accuracy of the network on the 10000 test images: 44 %\n",
            "Accuracy of the network on the 10000 test images: 11 %\n",
            "TEST ON NOISY DATA WITH STD OF 0.075\n",
            "Accuracy of the network on the 10000 test images: 36 %\n",
            "Accuracy of the network on the 10000 test images: 24 %\n",
            "Accuracy of the network on the 10000 test images: 35 %\n",
            "Accuracy of the network on the 10000 test images: 10 %\n",
            "TEST ON NOISY DATA WITH STD OF 0.1\n",
            "Accuracy of the network on the 10000 test images: 31 %\n",
            "Accuracy of the network on the 10000 test images: 19 %\n",
            "Accuracy of the network on the 10000 test images: 30 %\n",
            "Accuracy of the network on the 10000 test images: 11 %\n",
            "TEST ON NOISY DATA WITH STD OF 0.125\n",
            "Accuracy of the network on the 10000 test images: 28 %\n",
            "Accuracy of the network on the 10000 test images: 16 %\n",
            "Accuracy of the network on the 10000 test images: 26 %\n",
            "Accuracy of the network on the 10000 test images: 11 %\n",
            "TEST ON NOISY DATA WITH STD OF 0.15\n",
            "Accuracy of the network on the 10000 test images: 25 %\n",
            "Accuracy of the network on the 10000 test images: 15 %\n",
            "Accuracy of the network on the 10000 test images: 24 %\n",
            "Accuracy of the network on the 10000 test images: 12 %\n",
            "TEST ON NOISY DATA WITH STD OF 0.2\n",
            "Accuracy of the network on the 10000 test images: 23 %\n",
            "Accuracy of the network on the 10000 test images: 14 %\n",
            "Accuracy of the network on the 10000 test images: 21 %\n",
            "Accuracy of the network on the 10000 test images: 14 %\n",
            "TEST ON NOISY DATA WITH STD OF 0.3\n",
            "Accuracy of the network on the 10000 test images: 19 %\n",
            "Accuracy of the network on the 10000 test images: 14 %\n",
            "Accuracy of the network on the 10000 test images: 19 %\n",
            "Accuracy of the network on the 10000 test images: 19 %\n",
            "TEST ON NOISY DATA WITH STD OF 0.4\n",
            "Accuracy of the network on the 10000 test images: 15 %\n",
            "Accuracy of the network on the 10000 test images: 16 %\n",
            "Accuracy of the network on the 10000 test images: 16 %\n",
            "Accuracy of the network on the 10000 test images: 14 %\n",
            "TEST ON NOISY DATA WITH STD OF 0.5\n",
            "Accuracy of the network on the 10000 test images: 13 %\n",
            "Accuracy of the network on the 10000 test images: 17 %\n",
            "Accuracy of the network on the 10000 test images: 14 %\n",
            "Accuracy of the network on the 10000 test images: 10 %\n",
            "TEST ON NOISY DATA WITH STD OF 0.6\n",
            "Accuracy of the network on the 10000 test images: 13 %\n",
            "Accuracy of the network on the 10000 test images: 16 %\n",
            "Accuracy of the network on the 10000 test images: 13 %\n",
            "Accuracy of the network on the 10000 test images: 10 %\n",
            "TEST ON NOISY DATA WITH STD OF 0.7\n",
            "Accuracy of the network on the 10000 test images: 12 %\n",
            "Accuracy of the network on the 10000 test images: 14 %\n",
            "Accuracy of the network on the 10000 test images: 12 %\n",
            "Accuracy of the network on the 10000 test images: 10 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        outputs = net(images)\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "id": "uAQw0IcjEDdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f66e6ea2-d556-417a-dfe1-e62f19914960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 83 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        outputs = encoder1_prune3(images)\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "id": "kMq5zaWXzxaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14c8fe66-f6d9-48ed-c0c7-5ecee358d89e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 85 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        outputs = encoder1_prune4(images)\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "id": "Bxuqwh5hEGZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "108045b6-b9ce-4bf6-9cf0-bcae095d599f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 85 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0], data[1]\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          img = images[j] + (0.2**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images).to(device)\n",
        "\n",
        "        outputs = net(images)\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.to(device)).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "id": "ZUNrFuriFofW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "442605c5-01c7-4749-b310-fdd9fdc70825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 21 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0], data[1]\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          img = images[j] + (0.2**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images).to(device)\n",
        "\n",
        "        outputs = encoder1_prune3(images)\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.to(device)).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "id": "F1bHM9B-Kxyo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7160e7d5-257f-43ca-a49e-2a5355d0ad72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 20 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0], data[1]\n",
        "        all_images = []\n",
        "        for j in range(len(images)):\n",
        "          torch.manual_seed(1)\n",
        "          img = images[j] + (0.2**0.5)*torch.randn(3, 32, 32)\n",
        "          all_images.append(img)\n",
        "        images = torch.stack(all_images).to(device)\n",
        "\n",
        "        outputs = encoder1_prune4(images)\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.to(device)).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "id": "Ij42YBDNEJsn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bcf831a-4b8f-4fc0-bf7e-9d28deb8956d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 22 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "images, labels = next(dataiter)\n",
        "images, labels = next(dataiter)\n",
        "# images, labels = next(dataiter)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# print images\n",
        "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02286721-9516-4b71-9207-ee4b0d916686",
        "id": "GZJMqolbLXGT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GroundTruth:  dog   horse truck ship \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = encoder1_prune3(images)"
      ],
      "metadata": {
        "id": "v_ZyytnGLXGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88149a96-5ace-4315-c7a4-fafe9166fa5a",
        "id": "cMbmRND7LXGT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.6242, -2.6016,  1.4493,  3.2349,  1.3555,  2.2707,  4.1789, -0.3382,\n",
              "         -2.4464, -2.6534],\n",
              "        [-0.7002,  0.1009,  0.4036,  1.6516, -0.1371,  1.2689, -1.2314,  7.3374,\n",
              "         -7.5172,  3.0268],\n",
              "        [ 1.0395,  3.6198,  0.8230, -0.1761, -2.8255, -1.0305, -0.9685, -0.9082,\n",
              "          0.5422,  6.3055],\n",
              "        [ 1.3417,  3.3315,  0.1680,  0.0112, -0.5911, -2.1080,  4.1196, -4.0833,\n",
              "          2.6733,  1.2527]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = encoder1_prune3(all_images)\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd93rh6KuZjY",
        "outputId": "aca6a14a-792f-4575-fe89-96eccfc43cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.6601, -2.5661,  1.4749,  3.1882,  1.5135,  2.1588,  4.1885, -0.2737,\n",
              "         -2.4662, -2.7237],\n",
              "        [-0.6108,  0.2329,  0.4483,  1.5867, -0.1537,  1.2012, -1.2451,  7.2986,\n",
              "         -7.5479,  3.2231],\n",
              "        [ 0.9055,  3.5106,  0.8045, -0.2396, -2.6707, -0.8662, -1.1210, -0.6645,\n",
              "          0.3587,  6.1267],\n",
              "        [ 1.1772,  3.2969,  0.2337,  0.1213, -0.5985, -2.0766,  4.1281, -4.0856,\n",
              "          2.7369,  1.1493]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor([[-2.8701, -1.5173,  0.8548,  1.0653,  1.3374,  2.8186,  1.8546,  1.2185,\n",
        "         -3.4150, -2.2547],\n",
        "        [-3.6564, -2.5548, -2.7410,  2.4404,  1.8596,  4.2753, -4.9454, 13.3275,\n",
        "         -7.5260, -2.7180],\n",
        "        [-0.2248,  2.4604,  1.1115, -0.8863, -3.3311, -1.6360, -3.8775, -0.3140,\n",
        "         -0.5679,  7.1845],\n",
        "        [ 0.1122,  0.3341,  0.5881, -0.7892,  0.2279, -1.1876,  1.6933, -2.4103,\n",
        "          0.8221, -0.2586]], grad_fn=<AddmmBackward0>)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "jYdrU_e0LhtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idea - why not see what the top 5 most probable classes are when predicting each class, see how they change as we prune."
      ],
      "metadata": {
        "id": "L4h2yjMpwnlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing for the main network\n",
        "\n",
        "import collections\n",
        "from collections import defaultdict\n",
        "\n",
        "dist_list = [defaultdict(int), defaultdict(int), defaultdict(int), defaultdict(int), defaultdict(int), defaultdict(int), defaultdict(int), defaultdict(int), defaultdict(int), defaultdict(int)]\n",
        "\n",
        "dictionary_of_labels = {0:'plane', 1:'car', 2:'bird', 3:'cat',\n",
        "           4:'deer', 5:'dog', 6:'frog', 7:'horse', 8:'ship', 9:'truck'}\n",
        "\n",
        "for i, data in enumerate(testloader, 0):\n",
        "  inputs, labels = data\n",
        "  # print(labels)\n",
        "  outputs = net(inputs)\n",
        "  for j in range(4):\n",
        "    # print(torch.topk(outputs[j], 5).indices)\n",
        "    for k in torch.topk(outputs[j], 5).indices:\n",
        "      dist_list[labels[j]][dictionary_of_labels[k.item()]] += 1\n",
        "  # print(outputs)"
      ],
      "metadata": {
        "id": "-FsvgVrGwoJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for i in dist_list:\n",
        "  print(dictionary_of_labels[count], end = ' ')\n",
        "  print(sorted(i.items(), key = lambda x:-x[1]))\n",
        "  count += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC3eC716yBfp",
        "outputId": "bbf10a09-8c11-4584-91b2-dc36b4fcc157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "plane [('plane', 969), ('bird', 836), ('ship', 640), ('car', 613), ('truck', 565), ('deer', 524), ('horse', 366), ('cat', 262), ('dog', 171), ('frog', 54)]\n",
            "car [('car', 965), ('truck', 931), ('plane', 824), ('ship', 682), ('bird', 632), ('frog', 299), ('horse', 244), ('cat', 170), ('deer', 160), ('dog', 93)]\n",
            "bird [('bird', 953), ('deer', 785), ('dog', 763), ('cat', 680), ('horse', 576), ('plane', 455), ('frog', 428), ('ship', 137), ('car', 119), ('truck', 104)]\n",
            "cat [('cat', 910), ('dog', 861), ('bird', 790), ('deer', 746), ('horse', 575), ('frog', 458), ('plane', 201), ('car', 165), ('ship', 149), ('truck', 145)]\n",
            "deer [('deer', 961), ('bird', 898), ('dog', 820), ('horse', 728), ('cat', 652), ('frog', 483), ('plane', 250), ('truck', 72), ('car', 69), ('ship', 67)]\n",
            "dog [('dog', 942), ('cat', 912), ('bird', 812), ('horse', 795), ('deer', 709), ('frog', 286), ('plane', 162), ('ship', 143), ('car', 137), ('truck', 102)]\n",
            "frog [('frog', 920), ('bird', 905), ('deer', 808), ('cat', 800), ('dog', 784), ('horse', 282), ('car', 160), ('plane', 133), ('truck', 126), ('ship', 82)]\n",
            "horse [('horse', 971), ('dog', 911), ('deer', 872), ('cat', 762), ('bird', 698), ('truck', 260), ('plane', 256), ('car', 136), ('frog', 80), ('ship', 54)]\n",
            "ship [('ship', 954), ('plane', 926), ('truck', 839), ('car', 832), ('bird', 718), ('deer', 261), ('horse', 186), ('cat', 140), ('dog', 95), ('frog', 49)]\n",
            "truck [('truck', 945), ('car', 865), ('plane', 727), ('horse', 630), ('ship', 452), ('bird', 407), ('cat', 368), ('dog', 262), ('deer', 205), ('frog', 139)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the small network"
      ],
      "metadata": {
        "id": "j04VfZZJ3ijp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing for the small network\n",
        "\n",
        "import collections\n",
        "from collections import defaultdict\n",
        "\n",
        "other_dist_list = [defaultdict(int), defaultdict(int), defaultdict(int), defaultdict(int), defaultdict(int), defaultdict(int), defaultdict(int), defaultdict(int), defaultdict(int), defaultdict(int)]\n",
        "\n",
        "dictionary_of_labels = {0:'plane', 1:'car', 2:'bird', 3:'cat',\n",
        "           4:'deer', 5:'dog', 6:'frog', 7:'horse', 8:'ship', 9:'truck'}\n",
        "\n",
        "for i, data in enumerate(testloader, 0):\n",
        "  inputs, labels = data\n",
        "  # print(labels)\n",
        "  outputs = encoder1_prune3(inputs)\n",
        "  for j in range(4):\n",
        "    # print(torch.topk(outputs[j], 5).indices)\n",
        "    for k in torch.topk(outputs[j], 5).indices:\n",
        "      other_dist_list[labels[j]][dictionary_of_labels[k.item()]] += 1\n",
        "  # print(outputs)"
      ],
      "metadata": {
        "id": "UtjeogSY3vf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(other_dist_list)):\n",
        "  print(dictionary_of_labels[i])\n",
        "  print('pre-pruning', end = ' ')\n",
        "  print(sorted(dist_list[i].items(), key = lambda x:-x[1]))\n",
        "  print('post-pruning', end = ' ')\n",
        "  print(sorted(other_dist_list[i].items(), key = lambda x:-x[1]))\n",
        "\n",
        "# count = 0\n",
        "# for i in dist_list:\n",
        "#   print(dictionary_of_labels[count], end = ' ')\n",
        "#   print(sorted(i.items(), key = lambda x:-x[1]))\n",
        "#   count += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axHCTxgd3yI5",
        "outputId": "54231494-e38c-4ac3-dfa8-9b133f9e80ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "plane\n",
            "pre-pruning [('plane', 969), ('bird', 836), ('ship', 640), ('car', 613), ('truck', 565), ('deer', 524), ('horse', 366), ('cat', 262), ('dog', 171), ('frog', 54)]\n",
            "post-pruning [('plane', 961), ('bird', 803), ('ship', 737), ('truck', 613), ('deer', 595), ('car', 507), ('cat', 369), ('horse', 151), ('dog', 134), ('frog', 130)]\n",
            "car\n",
            "pre-pruning [('car', 965), ('truck', 931), ('plane', 824), ('ship', 682), ('bird', 632), ('frog', 299), ('horse', 244), ('cat', 170), ('deer', 160), ('dog', 93)]\n",
            "post-pruning [('car', 979), ('truck', 958), ('plane', 804), ('ship', 686), ('cat', 448), ('bird', 412), ('frog', 332), ('deer', 167), ('horse', 129), ('dog', 85)]\n",
            "bird\n",
            "pre-pruning [('bird', 953), ('deer', 785), ('dog', 763), ('cat', 680), ('horse', 576), ('plane', 455), ('frog', 428), ('ship', 137), ('car', 119), ('truck', 104)]\n",
            "post-pruning [('bird', 952), ('cat', 849), ('deer', 736), ('dog', 684), ('frog', 484), ('plane', 461), ('horse', 433), ('ship', 160), ('truck', 147), ('car', 94)]\n",
            "cat\n",
            "pre-pruning [('cat', 910), ('dog', 861), ('bird', 790), ('deer', 746), ('horse', 575), ('frog', 458), ('plane', 201), ('car', 165), ('ship', 149), ('truck', 145)]\n",
            "post-pruning [('cat', 958), ('dog', 882), ('bird', 731), ('deer', 658), ('horse', 551), ('frog', 533), ('plane', 220), ('truck', 200), ('car', 142), ('ship', 125)]\n",
            "deer\n",
            "pre-pruning [('deer', 961), ('bird', 898), ('dog', 820), ('horse', 728), ('cat', 652), ('frog', 483), ('plane', 250), ('truck', 72), ('car', 69), ('ship', 67)]\n",
            "post-pruning [('deer', 961), ('bird', 844), ('cat', 756), ('dog', 702), ('horse', 662), ('frog', 549), ('plane', 256), ('truck', 102), ('ship', 92), ('car', 76)]\n",
            "dog\n",
            "pre-pruning [('dog', 942), ('cat', 912), ('bird', 812), ('horse', 795), ('deer', 709), ('frog', 286), ('plane', 162), ('ship', 143), ('car', 137), ('truck', 102)]\n",
            "post-pruning [('cat', 954), ('dog', 942), ('bird', 774), ('horse', 769), ('deer', 667), ('frog', 401), ('plane', 167), ('truck', 146), ('ship', 104), ('car', 76)]\n",
            "frog\n",
            "pre-pruning [('frog', 920), ('bird', 905), ('deer', 808), ('cat', 800), ('dog', 784), ('horse', 282), ('car', 160), ('plane', 133), ('truck', 126), ('ship', 82)]\n",
            "post-pruning [('frog', 949), ('cat', 872), ('bird', 847), ('deer', 725), ('dog', 674), ('horse', 239), ('car', 236), ('truck', 235), ('plane', 141), ('ship', 82)]\n",
            "horse\n",
            "pre-pruning [('horse', 971), ('dog', 911), ('deer', 872), ('cat', 762), ('bird', 698), ('truck', 260), ('plane', 256), ('car', 136), ('frog', 80), ('ship', 54)]\n",
            "post-pruning [('horse', 942), ('dog', 852), ('deer', 826), ('cat', 776), ('bird', 672), ('truck', 368), ('plane', 207), ('frog', 185), ('car', 129), ('ship', 43)]\n",
            "ship\n",
            "pre-pruning [('ship', 954), ('plane', 926), ('truck', 839), ('car', 832), ('bird', 718), ('deer', 261), ('horse', 186), ('cat', 140), ('dog', 95), ('frog', 49)]\n",
            "post-pruning [('ship', 958), ('plane', 912), ('truck', 836), ('car', 699), ('bird', 688), ('deer', 346), ('cat', 317), ('dog', 87), ('horse', 82), ('frog', 75)]\n",
            "truck\n",
            "pre-pruning [('truck', 945), ('car', 865), ('plane', 727), ('horse', 630), ('ship', 452), ('bird', 407), ('cat', 368), ('dog', 262), ('deer', 205), ('frog', 139)]\n",
            "post-pruning [('truck', 969), ('car', 891), ('plane', 690), ('cat', 641), ('ship', 483), ('horse', 469), ('bird', 296), ('dog', 218), ('deer', 179), ('frog', 164)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plane [('plane', 969), ('bird', 836), ('ship', 640), ('car', 613), ('truck', 565), ('deer', 524), ('horse', 366), ('cat', 262), ('dog', 171), ('frog', 54)]\n",
        "car [('car', 965), ('truck', 931), ('plane', 824), ('ship', 682), ('bird', 632), ('frog', 299), ('horse', 244), ('cat', 170), ('deer', 160), ('dog', 93)]\n",
        "bird [('bird', 953), ('deer', 785), ('dog', 763), ('cat', 680), ('horse', 576), ('plane', 455), ('frog', 428), ('ship', 137), ('car', 119), ('truck', 104)]\n",
        "cat [('cat', 910), ('dog', 861), ('bird', 790), ('deer', 746), ('horse', 575), ('frog', 458), ('plane', 201), ('car', 165), ('ship', 149), ('truck', 145)]\n",
        "deer [('deer', 961), ('bird', 898), ('dog', 820), ('horse', 728), ('cat', 652), ('frog', 483), ('plane', 250), ('truck', 72), ('car', 69), ('ship', 67)]\n",
        "dog [('dog', 942), ('cat', 912), ('bird', 812), ('horse', 795), ('deer', 709), ('frog', 286), ('plane', 162), ('ship', 143), ('car', 137), ('truck', 102)]\n",
        "frog [('frog', 920), ('bird', 905), ('deer', 808), ('cat', 800), ('dog', 784), ('horse', 282), ('car', 160), ('plane', 133), ('truck', 126), ('ship', 82)]\n",
        "horse [('horse', 971), ('dog', 911), ('deer', 872), ('cat', 762), ('bird', 698), ('truck', 260), ('plane', 256), ('car', 136), ('frog', 80), ('ship', 54)]\n",
        "ship [('ship', 954), ('plane', 926), ('truck', 839), ('car', 832), ('bird', 718), ('deer', 261), ('horse', 186), ('cat', 140), ('dog', 95), ('frog', 49)]\n",
        "truck [('truck', 945), ('car', 865), ('plane', 727), ('horse', 630), ('ship', 452), ('bird', 407), ('cat', 368), ('dog', 262), ('deer', 205), ('frog', 139)]"
      ],
      "metadata": {
        "id": "FzrZJtFp4NrN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}